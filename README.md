<h1 align="center">
  <img src="https://upload.wikimedia.org/wikipedia/commons/b/b7/Technion_logo.svg" alt="Technion Logo" height="100">
  <br>
  Advanced Information Retrieval - Final Project
</h1>

<p align="center">
  <em>
    Query Adaptive Contextual Document Embeddings
  </em>
</p>

<p align="center">
  <strong>Technion - Israel Institute of Technology</strong> <br>
  Faculty of Data Science and Decisions
</p>

<h1 align="center">
  <img src="https://github.com/shoshosho3/query_adaptive_contextual_document_embedding/blob/main/pictures_QACDE/IR_Logo_1.png" alt="IR Logo 1" height="200">
  <img src="https://github.com/shoshosho3/query_adaptive_contextual_document_embedding/blob/main/pictures_QACDE/IR_Logo_2.png" alt="IR Logo 2" height="200">
</h1>

---

<details open>
<summary><strong>Table of Contents</strong> ⚙️</summary>

1. [About the Article](#link-of-the-article)
2. [Project Overview](#project-overview)  
3. [About the code](#about-the-code)
4. [Running Instructions](#running-instructions)  
5. [Results & Comments](#results-&-comments)  

</details>

---

## About the article
Our project is an extension on the following article:
<blockquote>
  <a href="https://arxiv.org/abs/2410.02525">Contextual Document Embeddings</a> by John X. Morris and Alexander M. Rush.</blockquote>

The article "Contextual Document Embeddings" by John X. Morris and Alexander M. Rush from Cornell University proposes a method to improve document embeddings by incorporating context from surrounding documents. Traditional embeddings, which rely solely on the individual document, can be insufficient for highly specific information retrieval tasks. This paper addresses this by incorporating neighboring documents into the embedding process, making the representation more context-aware. The authors introduce a contextual training objective and an architecture that explicitly encodes information from neighboring documents, demonstrating improved performance in various scenarios.

## Project Overview
In this project, we aimed to extend the "Contextual Document Embeddings" model by introducing a query-adaptive approach. While the original model effectively contextualizes document embeddings by considering neighboring documents, it does not account for the user's needs expressed in the query, which is essential for information retrieval tasks.

To achieve this, we developed two models. The first model, called Query Adaptive Contextual Document Embedder (QACDE), makes the document embedding query-adaptive, meaning that the document embedding is influenced by the query's embedding, following the same embedding method proposed in the original paper. However, upon reflection, we realized that the document embedding greatly depends on how the query is represented. This led us to develop a second model that computes the document embedding using multiple query embeddings, such as BERT, TF-IDF and query embedding generated by the model of the article. The goal of this approach is to combine the strengths of different query embedding methods to create more robust and informative document embeddings. By leveraging multiple representations of the query, we aim to capture various aspects of the document-query relationship, improving the relevance of the information retrieval process.


## About the code
The code for this project is built upon the original implementation from the paper. We first use the pre-trained model from the article to generate document and query embeddings. These embeddings are then utilized to train our own models. Our code handles the training and evaluation of both our query-adaptive models as well as the evaluation of the original model from the paper (which serves as our baseline). Additionally, it manages the query embedding generation using TF-IDF and BERT methods. The evaluation is conducted using Mean Average Precision (MAP), with the goal of comparing the performance of our models and the baseline across various BeIR datasets such as SciFact, FiQA, and NFCorpus. The training procedure includes contrastive negative sampling and a custom Loss function tailored to the specific training conditions, ensuring a thorough and meaningful comparison between the models.


## Running Instructions
This repository contains the code and instructions to run the code. It is preferable to have acces to a Virtual Machine with a GPU because of the computational cost of the models.

1. Download and install the dependencies by running the following command in your terminal:
```python
pip install -r requirements.txt
```

2. Run the following command in your terminal to execute the experiment:
```python
python "your_local_path"/project_files/main.py -K 20 -T 10000 -seed 42 -fixed_delay 10 -num_sim 250
```
Those hyperparameters match exactly those of the paper's experiment.

4. You can modify the values of the following parameters to generate different simulations:

- `-K`: The number of arms of the simulation. For example, `-K 20` will run a simulation with 20 arms.
- `-T`: The time duration for each simulation in arbitrary units. For example, `-T 10000` will run each simulation for 10,000 timeslots.
- `-seed`: The random seed for reproducibility. For example, `-seed 42` will set the seed to 42.
- `-fixed_delay`: The value of the delay for the simulation with fixed delay. For example, `-fixed_delay 10` sets a fixed delay of 10 timeslots.
- `-num_sim`: The number of individual simulations to run. For example, `-num_sim 250` will run 250 simulations.

Feel free to adjust these values to explore different scenarios or to generate alternative results based on your preferences.


## Results & Comments
Here are the graphs generated by the simulation:
<h1 align="center">
  <img src="https://github.com/tombijaoui/Sequential-Decision-Making-Project/blob/main/pictures_MAB/Cumulative%20Rewards.png" alt="Results" height="350">
  <img src="https://github.com/tombijaoui/Sequential-Decision-Making-Project/blob/main/pictures_MAB/Cumulative%20Regrets.png" alt="Cumulative Regrets" height="350">
</h1>

1. **Cumulative Reward**: 
   - Simulations with smaller delays achieve higher cumulative rewards compared to those with fixed or larger delays. This is because smaller delays allow more frequent optimal arm pulls, increasing exploitation.
   - All cumulative rewards grow linearly with time, likely due to the low variance in arm rewards.
   - The Oracle algorithm consistently outperforms UCB since it knows the optimal arm at each step, unlike UCB, which must estimate it. However, with longer delays, UCB and Oracle results converge, as Oracle is forced to explore suboptimal arms.

2. **Cumulative Regret**: 
   - Cumulative regret decreases over time across all simulations, as UCB explores enough to estimate rewards more accurately.
   - Lower delays lead to higher cumulative regret due to the difficulty in exploration, potentially resulting in premature suboptimal decisions.
   - Slight negative cumulative regret at the end suggests UCB's arm selection surpasses Oracle's greedy strategy in some cases.

